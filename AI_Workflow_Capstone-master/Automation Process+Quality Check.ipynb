{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case study objectives\n",
    "\n",
    "1. Gather all relevant data from the sources \n",
    "2. Implement several checks for quality assurance \n",
    "3. Automating of the ingestion pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gathering the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import libraries\n",
    "import os\n",
    "from datetime import datetime\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "\n",
    "# specify the directory you saved the data in\n",
    "data_dir = os.path.join(\".\",\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_db(file_path):\n",
    "    try:\n",
    "        conn = sqlite3.connect(file_path)\n",
    "        print(\"...successfully connected to db\\n\")\n",
    "    except Error as e:\n",
    "        print(\"...unsuccessful connection\\n\",e)\n",
    "    \n",
    "    return(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CUSTOMER', 'INVOICE', 'INVOICE_ITEM', 'COUNTRY']\n"
     ]
    }
   ],
   "source": [
    "## connect to the database\n",
    "conn = connect_db(os.path.join(data_dir,\"aavail-customers.db\"))\n",
    "\n",
    "## take a look of the tables when connecting the database\n",
    "tables = [t[0] for t in conn.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")]\n",
    "print(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>last_name</th>\n",
       "      <th>first_name</th>\n",
       "      <th>DOB</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Todd</td>\n",
       "      <td>Kasen</td>\n",
       "      <td>07/30/98</td>\n",
       "      <td>Rock Hill</td>\n",
       "      <td>South Carolina</td>\n",
       "      <td>united_states</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Garza</td>\n",
       "      <td>Ensley</td>\n",
       "      <td>04/12/89</td>\n",
       "      <td>singapore</td>\n",
       "      <td>None</td>\n",
       "      <td>singapore</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Carey</td>\n",
       "      <td>Lillian</td>\n",
       "      <td>09/12/97</td>\n",
       "      <td>Auburn</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>united_states</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Christensen</td>\n",
       "      <td>Beau</td>\n",
       "      <td>01/28/99</td>\n",
       "      <td>Hempstead</td>\n",
       "      <td>New York</td>\n",
       "      <td>united_states</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Gibson</td>\n",
       "      <td>Ernesto</td>\n",
       "      <td>03/23/98</td>\n",
       "      <td>singapore</td>\n",
       "      <td>None</td>\n",
       "      <td>singapore</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id    last_name first_name       DOB       city           state  \\\n",
       "0            1         Todd      Kasen  07/30/98  Rock Hill  South Carolina   \n",
       "1            2        Garza     Ensley  04/12/89  singapore            None   \n",
       "2            3        Carey    Lillian  09/12/97     Auburn         Alabama   \n",
       "3            4  Christensen       Beau  01/28/99  Hempstead        New York   \n",
       "4            5       Gibson    Ernesto  03/23/98  singapore            None   \n",
       "\n",
       "         country gender  \n",
       "0  united_states      m  \n",
       "1      singapore      f  \n",
       "2  united_states      f  \n",
       "3  united_states      m  \n",
       "4      singapore      m  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sql query to retrieve the needed data\n",
    "query = \"\"\"\n",
    "SELECT cu.customer_id, cu.last_name, cu.first_name, cu.DOB,\n",
    "       cu.city, cu.state, co.country_name, cu.gender\n",
    "FROM CUSTOMER cu\n",
    "INNER JOIN COUNTRY co\n",
    "ON cu.country_id = co.country_id;\n",
    "\"\"\"\n",
    "\n",
    "_data = [d for d in conn.execute(query)]\n",
    "columns = [\"customer_id\",\"last_name\",\"first_name\",\"DOB\",\"city\",\"state\",\"country\",\"gender\"]\n",
    "df_db = pd.DataFrame(_data,columns=columns)\n",
    "##\n",
    "df_db.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Extracting the relevant data from the CSV file as needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18859, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>stream_id</th>\n",
       "      <th>date</th>\n",
       "      <th>invoice_item_id</th>\n",
       "      <th>subscription_stopped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1420.0</td>\n",
       "      <td>2018-10-21</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1343.0</td>\n",
       "      <td>2018-10-23</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1756.0</td>\n",
       "      <td>2018-11-05</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1250.0</td>\n",
       "      <td>2018-11-06</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1324.0</td>\n",
       "      <td>2018-11-12</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_id  stream_id        date  invoice_item_id  subscription_stopped\n",
       "0            1     1420.0  2018-10-21              2.0                     0\n",
       "1            1     1343.0  2018-10-23              2.0                     0\n",
       "2            1     1756.0  2018-11-05              2.0                     0\n",
       "3            1     1250.0  2018-11-06              2.0                     0\n",
       "4            1     1324.0  2018-11-12              2.0                     0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## retrieve the relevant data from a csv file\n",
    "df_streams = pd.read_csv(os.path.join(\".\",r\"Data/aavail-streams.csv\"))\n",
    "print(df_streams.shape)\n",
    "df_streams.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating a churn table \n",
    "customer_ids = df_streams['customer_id'].values\n",
    "unique_ids = np.unique(df_streams['customer_id'].values)\n",
    "streams = df_streams['subscription_stopped'].values\n",
    "has_churned = [0 if streams[customer_ids==uid].max() > 0 else 1 for uid in unique_ids]\n",
    "df_churn = pd.DataFrame({\"customer_id\": unique_ids,\"is_subscriber\": has_churned})\n",
    "\n",
    "#df_churn.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checks for quality assurance\n",
    "\n",
    "Sometimes it is known in advance which types of data integrity issues to expect, but other times it is during the Exploratory Data Analysis (EDA) process that these issues are identified.  After extracting data it is important to include checks for quality assurance even on the first pass through the AI workflow. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaning Summary\n",
      "-----------------------------------\n",
      "Removed 7 duplicate rows\n",
      "Removed 1164 missing stream ids\n",
      "\n",
      "Missing Value Summary\n",
      "-----------------------------------\n",
      "\n",
      "df_db\n",
      "---------------\n",
      "customer_id      0\n",
      "last_name        0\n",
      "first_name       0\n",
      "DOB              0\n",
      "city             0\n",
      "state          300\n",
      "country          0\n",
      "gender           0\n",
      "dtype: int64\n",
      "\n",
      "df_streams\n",
      "---------------\n",
      "customer_id             0\n",
      "stream_id               0\n",
      "date                    0\n",
      "invoice_item_id         0\n",
      "subscription_stopped    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## check & display the results\n",
    "print(\"\\nCleaning Summary\\n{}\".format(\"-\"*35))\n",
    "duplicate_rows = df_db.duplicated()\n",
    "\n",
    "if True in duplicate_rows:\n",
    "    df_db = df_db[~duplicate_rows]\n",
    "print(\"Removed {} duplicate rows\".format(np.where(duplicate_rows==True)[0].size))\n",
    "\n",
    "missing_stream_ids = np.isnan(df_streams['stream_id'])    \n",
    "if True in missing_stream_ids:\n",
    "    df_streams = df_streams[~missing_stream_ids]\n",
    "    \n",
    "    \n",
    "print(\"Removed {} missing stream ids\".format(np.where(missing_stream_ids==True)[0].size))\n",
    "\n",
    "print(\"\\nMissing Value Summary\\n{}\".format(\"-\"*35))\n",
    "print(\"\\ndf_db\\n{}\".format(\"-\"*15))\n",
    "print(df_db.isnull().sum(axis = 0))\n",
    "print(\"\\ndf_streams\\n{}\".format(\"-\"*15))\n",
    "print(df_streams.isnull().sum(axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining the data into a single data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_churn.copy()\n",
    "df_clean = df_clean[np.in1d(df_clean['customer_id'].values,df_db['customer_id'].values)]\n",
    "unique_ids = df_clean['customer_id'].values\n",
    "\n",
    "## ensure we are working with correctly ordered customer_ids df_db\n",
    "if not np.array_equal(df_clean['customer_id'],df_db['customer_id']): \n",
    "    raise Exception(\"indexes are out of order or unmatched---needs to fix\")\n",
    "\n",
    "## query the db to create a invoice item map\n",
    "query = \"\"\"\n",
    "SELECT i.invoice_item_id, i.invoice_item\n",
    "FROM INVOICE_ITEM i;\n",
    "\"\"\"\n",
    "\n",
    "## variables for new df creation\n",
    "invoice_item_map = {d[0]:d[1] for d in conn.execute(query)}\n",
    "streams_stopped = df_streams['subscription_stopped'].values\n",
    "streams_cid = df_streams['customer_id'].values\n",
    "streams_iid = df_streams['invoice_item_id'].values\n",
    "subscriber_invoice_mode = [stats.mode(streams_iid[streams_cid==uid])[0][0] for uid in unique_ids]\n",
    "\n",
    "## create the new df\n",
    "df_clean['country'] = df_db['country']\n",
    "df_clean['age'] = np.datetime64('today') - df_db['DOB'].astype('datetime64')\n",
    "df_clean['customer_name'] = df_db['first_name'] + \" \" + df_db['last_name']\n",
    "df_clean['subscriber_type'] = [invoice_item_map[int(sim)] for sim in subscriber_invoice_mode]\n",
    "df_clean['num_streams'] = [streams_stopped[streams_cid==uid].size for uid in unique_ids]\n",
    "\n",
    "## convert age to days\n",
    "df_clean['age'] = [a.astype('timedelta64[Y]').astype(int) for a in df_clean['age'].values]\n",
    "\n",
    "#df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automating the process\n",
    "\n",
    "1. Save the cleaned, combined data as a CSV file.\n",
    "2. Create a function or class that performs all of the steps given a database file and a streams CSV file.\n",
    "3. Run the function in batches and write a check to ensure the same result.\n",
    "\n",
    "There would be some logic involved in the automation process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code to split the streams csv into batches\n",
    "df_all = pd.read_csv(os.path.join(\".\",\"data/aavail-streams.csv\"))\n",
    "half = int(round(df_all.shape[0] * 0.5))\n",
    "df_part1 = df_all[:half]\n",
    "df_part2 = df_all[half:]\n",
    "df_part1.to_csv(os.path.join(\".\",\"data/aavail-streams-1.csv\"),index=False)\n",
    "df_part2.to_csv(os.path.join(\".\",\"data/aavail-streams-2.csv\"),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automating the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting aavail-data-ingestor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile aavail-data-ingestor.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import getopt\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "\n",
    "DATA_DIR = os.path.join(\"..\",\"data\")\n",
    "\n",
    "def connect_db(file_path):\n",
    "    \"\"\"\n",
    "    function to connection to aavail database\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(file_path)\n",
    "        print(\"...successfully connected to db\")\n",
    "    except Error as e:\n",
    "        print(\"...unsuccessful connection\",e)\n",
    "    \n",
    "    return(conn)\n",
    "\n",
    "## passing the parameter \"conn\" from last output\n",
    "def ingest_db_data(conn):\n",
    "    \"\"\"\n",
    "    load and clean the db data\n",
    "    \"\"\"\n",
    "    \n",
    "    query = \"\"\"\n",
    "            SELECT cu.customer_id, cu.last_name, cu.first_name, cu.DOB,\n",
    "            cu.city, cu.state, co.country_name, cu.gender\n",
    "            FROM CUSTOMER cu\n",
    "            INNER JOIN COUNTRY co\n",
    "            ON cu.country_id = co.country_id;\n",
    "            \"\"\"\n",
    "    _data = [d for d in conn.execute(query)]\n",
    "    columns = [\"customer_id\",\"last_name\",\"first_name\",\"DOB\",\"city\",\"state\",\"country\",\"gender\"]\n",
    "    df_db = pd.DataFrame(_data,columns=columns)\n",
    "    ## the duplicated rows:\n",
    "    duplicate_rows = df_db.duplicated()\n",
    "    if True in duplicate_rows:\n",
    "        df_db = df_db[~duplicate_rows]\n",
    "        df_db.reset_index()\n",
    "    print(\"... removed {} duplicate rows in db data\".format(np.where(duplicate_rows==True)[0].size))\n",
    "    return(df_db)\n",
    "\n",
    "\n",
    "def ingest_stream_data(file_path):\n",
    "    \"\"\"\n",
    "    load and clean the stream data\n",
    "    \"\"\"\n",
    "    \n",
    "    df_streams = pd.read_csv(file_path)\n",
    "    customer_ids = df_streams['customer_id'].values\n",
    "    unique_ids = np.unique(df_streams['customer_id'].values)\n",
    "    streams = df_streams['subscription_stopped'].values\n",
    "    has_churned = [0 if streams[customer_ids==uid].max() > 0 else 1 for uid in unique_ids]\n",
    "    df_churn = pd.DataFrame({\"customer_id\": unique_ids,\"is_subscriber\": has_churned})\n",
    "    \n",
    "    missing_stream_ids = np.isnan(df_streams['stream_id'])    \n",
    "    if True in missing_stream_ids:\n",
    "        df_streams = df_streams[~missing_stream_ids]\n",
    "        df_streams.reset_index()\n",
    "    print(\"... removed {} missing stream ids\".format(np.where(missing_stream_ids==True)[0].size))\n",
    "    \n",
    "    return(df_streams,df_churn)\n",
    "\n",
    "\n",
    "def process_dataframes(df_streams,df_db,df_churn):\n",
    "    \"\"\"\n",
    "    add data to target csv\n",
    "    \"\"\"\n",
    "\n",
    "    df_clean = df_churn.copy()\n",
    "    df_db = df_db[np.in1d(df_db['customer_id'].values,df_clean['customer_id'].values)]\n",
    "    df_db.reset_index()\n",
    "    unique_ids = df_clean['customer_id'].values\n",
    "\n",
    "    ## ensure we are working with correctly ordered customer_ids df_db\n",
    "    if not np.array_equal(df_clean['customer_id'],df_db['customer_id']):\n",
    "        raise Exception(\"indexes are out of order or unmatched---needs to fix\")\n",
    "\n",
    "    ## query the db t create a invoice item map\n",
    "    query = \"\"\"\n",
    "    SELECT i.invoice_item_id, i.invoice_item\n",
    "    FROM INVOICE_ITEM i;\n",
    "    \"\"\"\n",
    "\n",
    "    ## variables for new df creation\n",
    "    invoice_item_map = {d[0]:d[1] for d in conn.execute(query)}\n",
    "    streams_stopped = df_streams['subscription_stopped'].values\n",
    "    streams_cid = df_streams['customer_id'].values\n",
    "    streams_iid = df_streams['invoice_item_id'].values\n",
    "    subscriber_invoice_mode = [stats.mode(streams_iid[streams_cid==uid])[0][0] for uid in unique_ids]\n",
    "\n",
    "    ## create the new df\n",
    "    df_clean['country'] = df_db['country']\n",
    "    df_clean['age'] = np.datetime64('today') - df_db['DOB'].astype('datetime64')\n",
    "    df_clean['age'] = [a.astype('timedelta64[Y]').astype(int) for a in df_clean['age'].values]\n",
    "    df_clean['customer_name'] = df_db['first_name'] + \" \" + df_db['last_name']\n",
    "    df_clean['subscriber_type'] = [invoice_item_map[int(sim)] for sim in subscriber_invoice_mode]\n",
    "    df_clean['num_streams'] = [streams_stopped[streams_cid==uid].size for uid in unique_ids]\n",
    "    \n",
    "    return(df_clean)\n",
    "\n",
    "## upate the target file    \n",
    "def update_target(target_file,dat,overwrite=False):\n",
    "    \"\"\"\n",
    "    update line by line in case data are large\n",
    "    \"\"\"\n",
    "\n",
    "    if overwrite or not os.path.exists(target_file):\n",
    "        dat.to_csv(target_file,index=False)   \n",
    "    else:\n",
    "        df_target = pd.read_csv(target_file)\n",
    "        df_target.to_csv(target_file, mode='a',index=False)\n",
    "        \n",
    "        \n",
    "         \n",
    "if __name__ == \"__main__\":\n",
    "  \n",
    "    ## collect args\n",
    "    arg_string = \"%s -d db_filepath -s streams_filepath\"%sys.argv[0]\n",
    "    try:\n",
    "        optlist, args = getopt.getopt(sys.argv[1:],'d:s:')\n",
    "    except getopt.GetoptError:\n",
    "        print(getopt.GetoptError)\n",
    "        raise Exception(arg_string)\n",
    "\n",
    "    ## handle args\n",
    "    streams_file = None\n",
    "    db_file = None\n",
    "    for o, a in optlist:\n",
    "        if o == '-d':\n",
    "            db_file = a\n",
    "        if o == '-s':\n",
    "            streams_file = a\n",
    "    streams_file = os.path.join(\".\",streams_file)\n",
    "    db_file = os.path.join(\".\",db_file)\n",
    "    target_file = os.path.join(\".\",\"aavail-target.csv\")\n",
    "    \n",
    "    \n",
    "    ## make the connection to the database\n",
    "    conn = connect_db(db_file)\n",
    "\n",
    "    ## ingest data base data\n",
    "    df_db = ingest_db_data(conn)\n",
    "    df_streams, df_churn = ingest_stream_data(streams_file)\n",
    "    df_clean = process_dataframes(df_streams, df_db, df_churn)\n",
    "    \n",
    "    ## write\n",
    "    update_target(target_file,dat,overwrite=False)\n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the script created from the commandline directly or from within this notebook using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...successfully connected to db\n",
      "... removed 7 duplicate rows in db data\n",
      "... removed 1164 missing stream ids\n",
      "   customer_id  is_subscriber  ...   subscriber_type  num_streams\n",
      "0            1              1  ...    aavail_premium           23\n",
      "1            2              0  ...  aavail_unlimited           12\n",
      "2            3              0  ...    aavail_premium           22\n",
      "3            4              1  ...      aavail_basic           19\n",
      "4            5              1  ...    aavail_premium           23\n",
      "\n",
      "[5 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "!python aavail-data-ingestor.py aavail-customers.db aavail-streams-1.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the script once for each batch and then load both the original and batch versions back into the notebook to check if they are the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...successfully connected to db\n",
      "... removed 7 duplicate rows in db data\n",
      "... removed 1164 missing stream ids\n",
      "done\n",
      "    1001 ../data/aavail-target.csv\n"
     ]
    }
   ],
   "source": [
    "!rm ../data/aavail-target.csv\n",
    "!python aavail-data-ingestor.py -d aavail-customers.db -s aavail-streams.csv\n",
    "!wc -l ../data/aavail-target.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...successfully connected to db\n",
      "... removed 7 duplicate rows in db data\n",
      "... removed 577 missing stream ids\n",
      "done\n",
      "     507 ../data/aavail-target.csv\n",
      "...successfully connected to db\n",
      "... removed 7 duplicate rows in db data\n",
      "... removed 587 missing stream ids\n",
      "done\n",
      "    1014 ../data/aavail-target.csv\n"
     ]
    }
   ],
   "source": [
    "!rm ../data/aavail-target.csv\n",
    "!python aavail-data-ingestor.py -d aavail-customers.db -s aavail-streams-1.csv\n",
    "!wc -l ../data/aavail-target.csv\n",
    "!python aavail-data-ingestor.py -d aavail-customers.db -s aavail-streams-2.csv\n",
    "!wc -l ../data/aavail-target.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
